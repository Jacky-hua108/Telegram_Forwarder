{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fmpBz0Cvun"
      },
      "source": [
        "# 0. Author notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh65UuO5Cvup"
      },
      "source": [
        "Hello everyone,\n",
        "\n",
        "Thank you for using my quick Facebook Scraping Python template.\n",
        "This solution leverages the library that called **facebook_scrapper** of **kevinzg**.\n",
        "\n",
        "You can learn more about this package at: https://pypi.org/project/facebook-scraper/\n",
        "\n",
        "I hope you can use this to develop your understanding in Social Listeners; Marketing; Market Insights; Sentiment Analysis;... and take this at the first step in Data Analytics world.\n",
        "\n",
        "I decide to share this works to my UFLL Vietnam (Unilever Future Leaders' League) fellows, friends, and you.\n",
        "\n",
        "Regards,\n",
        "Duc Lai\n",
        "\n",
        "📧: me@henryduc.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCXo99GlCvuq"
      },
      "source": [
        "# 1. Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEkUSyGsDC14",
        "outputId": "023ad6a3-7de5-4f98-facc-4bb54b01ca2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting facebook_scraper\n",
            "  Downloading facebook_scraper-0.2.59-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dateparser<2.0.0,>=1.0.0 (from facebook_scraper)\n",
            "  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting demjson3<4.0.0,>=3.0.5 (from facebook_scraper)\n",
            "  Downloading demjson3-3.0.6.tar.gz (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests-html<0.11.0,>=0.10.0 (from facebook_scraper)\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (2024.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser<2.0.0,>=1.0.0->facebook_scraper) (5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.32.3)\n",
            "Collecting pyquery (from requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting bs4 (from requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting w3lib (from requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2024.12.14)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (8.5.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.12.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook_scraper) (5.3.0)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html<0.11.0,>=0.10.0->facebook_scraper)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser<2.0.0,>=1.0.0->facebook_scraper) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html<0.11.0,>=0.10.0->facebook_scraper) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests-html<0.11.0,>=0.10.0->facebook_scraper) (2.6)\n",
            "Downloading facebook_scraper-0.2.59-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m170.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: demjson3\n",
            "  Building wheel for demjson3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for demjson3: filename=demjson3-3.0.6-py3-none-any.whl size=75281 sha256=59844d68320f1a53842eab9e4e4268256194a0f5712334322c9f62e0d6aa1990\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/32/af/75c810e14c7ca0df40c20c662fcc7f33d1e055937b4e452cc1\n",
            "Successfully built demjson3\n",
            "Installing collected packages: parse, demjson3, appdirs, websockets, w3lib, urllib3, pyee, fake-useragent, cssselect, pyquery, pyppeteer, dateparser, bs4, requests-html, facebook_scraper\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.1\n",
            "    Uninstalling websockets-14.1:\n",
            "      Successfully uninstalled websockets-14.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.2.3\n",
            "    Uninstalling urllib3-2.2.3:\n",
            "      Successfully uninstalled urllib3-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 0.3.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 dateparser-1.2.0 demjson3-3.0.6 facebook_scraper-0.2.59 fake-useragent-2.0.3 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.2.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "%pip install facebook_scraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wve5y32MCvur"
      },
      "source": [
        "# 2. Mapping Google Drive folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUwr7vDGDLv1",
        "outputId": "e4b76963-3708-4360-c22d-f7a9c37315a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOW9p1FoCvus"
      },
      "source": [
        "# 3. Define WebPage & Storage Link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqfpmr97DIJV"
      },
      "outputs": [],
      "source": [
        "FANPAGE_LINK =\"748121142964193\"\n",
        "FOLDER_PATH = \"/content/drive/MyDrive/AABET/\"\n",
        "COOKIE_PATH = \"/content/drive/MyDrive/AABET/www.facebook.com_cookies.txt\"\n",
        "\n",
        "PAGES_NUMBER = 5 # Posts-per-page = 4 => multiply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwSl1zwjCvus"
      },
      "source": [
        "# 4. Source Ready To Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYmaq7YQDJ2t"
      },
      "outputs": [],
      "source": [
        "from facebook_scraper import get_posts\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7CPqp2TCvut"
      },
      "source": [
        "### Crawling Start ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdrehHenDKyq"
      },
      "outputs": [],
      "source": [
        "post_list = []\n",
        "for post in get_posts(FANPAGE_LINK,\n",
        "                    options={\"comments\": True, \"reactions\": True, \"allow_extra_requests\": True},\n",
        "                    extra_info=True, pages=200, cookies=COOKIE_PATH):\n",
        "    post_list.append(post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_85GPEBCvut"
      },
      "source": [
        "### Post-processing & Output generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzwaAiHADsv2"
      },
      "outputs": [],
      "source": [
        "fields_list = ['post_id', 'text', 'time', 'images', 'images_description', 'likes', 'comments', 'shares', 'post_url', 'link', 'username']\n",
        "result_list = []\n",
        "for post in post_list:\n",
        "    my_obj = {}\n",
        "    for field in fields_list:\n",
        "        my_obj[field] = post[field]\n",
        "    ## Manipulate Comments\n",
        "    if len(post['comments_full']) > 0:\n",
        "        my_obj['comments_content'] = '\\n'.join([comment['comment_text'] for comment in post['comments_full']])\n",
        "        my_obj['comments_time'] = '\\n'.join([comment['comment_time'].strftime(\"%Y-%m-%d-%H:%M:%S\") for comment in post['comments_full']])\n",
        "\n",
        "    result_list.append(my_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eejp-tYuCvut"
      },
      "source": [
        "### 😎 Write it to CSV: <PAGE_NAME>.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B01vey7aDw3B"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(result_list)\n",
        "df.to_csv(FOLDER_PATH + FANPAGE_LINK + \".csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "FBCrawler_LTMDUC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}